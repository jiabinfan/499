import torch
import torch.nn as nn
import tensorflow as tf
from optimization import BertAdam

class NeuralLP(nn.Module):
    """ 
    pytorch version
    """

    def __init__(self, option):
        super(NeuralLP, self).__init__()
        self.seed = option.seed        
        self.num_step = option.num_step
        self.num_layer = option.num_layer
        self.rnn_state_size = option.rnn_state_size
        
        self.norm = not option.no_norm
        self.thr = option.thr
        self.dropout = option.dropout
        self.learning_rate = option.learning_rate
        self.accuracy = option.accuracy
        self.top_k = option.top_k
        
        self.num_entity = option.num_entity
        self.num_operator = option.num_operator
        self.query_is_language = option.query_is_language
        
        if not option.query_is_language:
            self.num_query = option.num_query
            self.query_embed_size = option.query_embed_size       
        else:
            self.vocab_embed_size = option.vocab_embed_size
            self.query_embed_size = self.vocab_embed_size
            self.num_vocab = option.num_vocab
            self.num_query = num_vocab
            self.num_word = option.num_word    

        self.qembedding =  nn.Embedding(self.num_query+1,self.query_embed_size)       
        self.qrnn = nn.LSTM(self.query_embed_size, self.rnn_state_size,1,batch_first =True)
        self.att_at = nn.Linear(self.rnn_state_size,self.num_operator) 

        self.criterion = nn.CrossEntropyLoss()

        self.device = torch.device("cuda" if torch.cuda.is_available() and not option.no_cuda else "cpu")
        self.to(self.device)
        # n_gpu = torch.cuda.device_count()

        print('-'*10+"Neual LP is built."+'-'*10)

    def forward(self, q,h,t, mdb):
        '''
        q,t,h: b_s,1
        mdb: 12 maps
        '''
        batch_size = len(q)
        tensorq = torch.tensor([q for i in range(self.num_step-1)] +[[self.num_query]*batch_size], dtype=torch.long).to(self.device) # (bs,)
        tensort = torch.tensor(t, dtype=torch.long).to(self.device)
        tensorh = torch.tensor(h, dtype=torch.long).to(self.device)
    
        databases = []
        for r in range(len(mdb)):
            ids = torch.LongTensor(mdb[r][0])
            values = torch.FloatTensor(mdb[r][1])
            map1 = torch.sparse.FloatTensor(ids.t(), values, mdb[r][2]).to(self.device) # (3007,3007)
            databases.append(map1)
            # print ids.size()
            # print values.size()
            # print map1.size()

        rnn_inputs = self.qembedding(tensorq.t()) # (bs,3,emb)
        h0 = torch.zeros(1, batch_size, self.rnn_state_size).to(self.device)
        c0 = torch.zeros(1, batch_size, self.rnn_state_size).to(self.device)
        rnn_outputs,(ht,ct) = self.qrnn(rnn_inputs, (h0,c0)) # (bs,3,hids)

        # a_t, (bs, 3, 24)
        a_t = self.att_at(rnn_outputs.contiguous().view(-1, self.rnn_state_size)).view(batch_size, self.num_step,
                self.num_operator)
        a_t = nn.Softmax(2)(a_t)

        # u_t, actually the tails at first
        u_ts = onehot(tensort, self.num_entity).unsqueeze(1) # (bs,1,3007)

        for t in range(self.num_step):
            # b, (bs,t,1)
            b = torch.bmm(rnn_outputs[:,:t+1,:] ,rnn_outputs[:,t:t+1,:].permute(0,2,1))
            b = nn.Softmax(1)(b) #

            # u_{t+1} = \sum b_t u_t, (bs,3007)
            u_tplus1 = torch.sum(b * u_ts,1)
            
            # construct  memories to store u_t
            if t < self.num_step - 1:
                # database_results: (will be) a list of num_operator tensors,
                # each of size (batch_size, num_entity).
                database_results = []    
                for r in xrange(self.num_operator/2): # 12
                    for op_matrix, op_attn in zip(
                                    [databases[r], 
                                     databases[r].transpose(1,0)], # 
                                    [a_t[:,t,r],  a_t[:,t,r+self.num_operator/2]]): # (bs,)
                        # M_R_k sum b_t u_t
                        product = torch.mm(u_tplus1,op_matrix.to_dense())  # (bs,3007)
                        # a_k M_R_k sum b_t _u_t
                        sumat = product*op_attn.unsqueeze(1)
                        database_results.append(sumat.unsqueeze(1))
                # u_t 
                added_database_results = torch.sum(torch.cat(database_results,1),1, keepdim = True) # 128,1,3007

                if self.norm:
                    added_database_results /= torch.max(torch.sum(added_database_results,2,
                        keepdim=True), torch.cuda.FloatTensor([self.thr]))
                
                if self.dropout > 0.:
                    added_database_results = nn.Dropout(self.dropout)(added_database_results)

                # Populate a new cell in memory by concatenating.  
                u_ts = torch.cat( 
                    [u_ts, added_database_results],1) #(bs,t+1,3007)
            else:
                predictions = u_tplus1 #(bs,3007)

        loss = - torch.mean(onehot(tensorh, self.num_entity) * torch.log(torch.max(predictions, \
                torch.cuda.FloatTensor([self.thr]))))
        # loss = self.criterion(predictions, tensorh)

        values,index_topk = torch.topk(predictions,self.top_k, sorted=False)      
        acc_res = torch.eq(index_topk, tensorh.unsqueeze(1))
        in_top = torch.sum(acc_res,1)
        return loss, in_top, predictions, index_topk
 
    def get_predictions_given_queries(self, qq, hh, tt, mdb):
        loss, in_top, predictions, top_k = self.forward(qq, hh, tt, mdb)
        return in_top.detach().cpu().numpy(), predictions.detach().cpu().numpy()
       
 
class LogicLearning(nn.Module):
    """ 
    pytorch version
    """

    def __init__(self, option):
        super(LogicLearning, self).__init__()
        self.seed = option.seed        
        self.num_step = option.num_step
        self.num_layer = option.num_layer
        self.rnn_state_size = option.rnn_state_size
        
        self.norm = not option.no_norm
        self.thr = option.thr
        self.dropout = option.dropout
        self.learning_rate = option.learning_rate
        self.accuracy = option.accuracy
        self.top_k = option.top_k
        
        self.num_entity = option.num_entity
        self.num_operator = option.num_operator
        self.query_is_language = option.query_is_language
        
        if not option.query_is_language:
            self.num_query = option.num_query
            self.query_embed_size = option.query_embed_size       
        else:
            self.vocab_embed_size = option.vocab_embed_size
            self.query_embed_size = self.vocab_embed_size
            self.num_vocab = option.num_vocab
            self.num_query = num_vocab
            self.num_word = option.num_word    

        self.qembedding =  nn.Embedding(self.num_query+1,self.query_embed_size)       
        self.qrnn = nn.LSTM(self.query_embed_size, self.rnn_state_size,1,batch_first =True)

        self.att_rel = nn.Linear(self.num_entity*2,self.num_operator) 
        self.attention_layers = [ nn.Linear(self.num_entity*2, self.num_operator) for i in
                range(self.num_step)]
        # self.attentions_layers = []
        for i,attention in enumerate(self.attention_layers):
                self.add_module('attention_layer_{}_{}'.format(i,i), attention) # important, add module to torch
            # self.attentions_layers.append(self.attentions)


        self.criterion = nn.CrossEntropyLoss()

        self.device = torch.device("cuda" if torch.cuda.is_available() and not option.no_cuda else "cpu")
        self.to(self.device)
        # n_gpu = torch.cuda.device_count()

        print('-'*10+"Neual LP is built."+'-'*10)

    def forward(self, q,h,t, mdb):
        '''
        q,t,h: b_s,1 (list
        mdb: 12 maps
        '''
        batch_size = len(q)
        tensorq = torch.tensor(q, dtype=torch.long).to(self.device)
        tensort = torch.tensor(t, dtype=torch.long).to(self.device)
        tensorh = torch.tensor(h, dtype=torch.long).to(self.device)
    
        databases = []
        for r in range(len(mdb)):
            ids = torch.LongTensor(mdb[r][0])
            values = torch.FloatTensor(mdb[r][1])
            map1 = torch.sparse.FloatTensor(ids.t(), values, mdb[r][2]).to(self.device) # (3007,3007)
            databases.append(map1)

        # print '-------------debug---------'

        u_l = onehot(tensort, self.num_entity) # (bs,1,3007)
        query = onehot(tensorq, self.num_entity) # (bs,1,3007)

        # for l in range(self.num_step):
        for al_att  in self.attention_layers:
            a_l = al_att(torch.cat([query,u_l],1))
            a_l = nn.Softmax(1)(a_l) # (bs,24)

            database_results = []    
            for r in xrange(self.num_operator/2): # 12
                for op_matrix, op_attn in zip(
                                [databases[r], 
                                 databases[r].transpose(1,0)], # 
                                [a_l[:,r],  a_l[:,r+self.num_operator/2]]): # (bs,)
                    # M_R_k sum u_t
                    product = torch.mm(u_l,op_matrix.to_dense())  # (bs,3007)
                    # a_k M_R_k sum b_t _u_t
                    sumat = product*op_attn.unsqueeze(1)
                    database_results.append(sumat.unsqueeze(1))
                # u_t 
            u_l = torch.sum(torch.cat(database_results,1),1, keepdim = False) # 128,1,3007

            if self.norm:
                u_l /= torch.max(torch.sum(u_l,1,
                    keepdim=True), torch.cuda.FloatTensor([self.thr]))


            if self.dropout > 0.:
                u_l = nn.Dropout(self.dropout)(u_l)

        predictions = u_l 
        loss = - torch.mean(onehot(tensorh, self.num_entity) * torch.log(torch.max(predictions, \
                torch.cuda.FloatTensor([self.thr]))))
        # loss = self.criterion(predictions, tensorh)

        values,index_topk = torch.topk(predictions,self.top_k, sorted=False)      
        acc_res = torch.eq(index_topk, tensorh.unsqueeze(1))
        in_top = torch.sum(acc_res,1)
        return loss, in_top, predictions, index_topk
 
    def get_predictions_given_queries(self, qq, hh, tt, mdb):
        loss, in_top, predictions, top_k = self.forward(qq, hh, tt, mdb)
        return in_top.detach().cpu().numpy(), predictions.detach().cpu().numpy()
  
class LogicLearning1(nn.Module):
    """ 
    pytorch version
    """

    def __init__(self, option):
        super(LogicLearning1, self).__init__()
        self.seed = option.seed        
        self.num_step = option.num_step
        self.num_layer = option.num_layer
        self.rnn_state_size = option.rnn_state_size
        
        self.norm = not option.no_norm
        self.thr = option.thr
        self.dropout = option.dropout
        self.learning_rate = option.learning_rate
        self.accuracy = option.accuracy
        self.top_k = option.top_k
        
        self.num_entity = option.num_entity
        self.num_operator = option.num_operator
        self.query_is_language = option.query_is_language
        
        if not option.query_is_language:
            self.num_query = option.num_query
            self.query_embed_size = option.query_embed_size       
        else:
            self.vocab_embed_size = option.vocab_embed_size
            self.query_embed_size = self.vocab_embed_size
            self.num_vocab = option.num_vocab
            self.num_query = num_vocab
            self.num_word = option.num_word    

        self.nhead = 8
        self.hids = 6
        self.attention_layers = [ nn.Linear(self.num_entity*2*self.nhead, self.num_operator*self.nhead) for i in
                range(self.num_step)]
        # self.attentions_layers = []

        for i,attention in enumerate(self.attention_layers):
                self.add_module('attention_layer_{}_{}'.format(i,i), attention) # important, add module to torch
            # self.attentions_layers.append(self.attentions)

        # self.clf = nn.Linear(self.num_entity*2, self.num_operator*self.nhead)
        self.criterion = nn.CrossEntropyLoss()

        self.fc_q = nn.Linear(self.num_entity,self.hids)
        self.fc1 = nn.Linear(self.num_entity,self.hids)
        self.fc2 = nn.Linear(self.hids*self.nhead,self.num_entity)

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        # n_gpu = torch.cuda.device_count()

        print('-'*10+"Neual LP is built."+'-'*10)

    def forward(self, tensorq,tensorh,tensort, mdb):
        '''
        q,t,h: b_s,1 (list
        mdb: 12 maps
        '''
        #tensorq = torch.tensor(q, dtype=torch.long).to(self.device)
        # tensort = torch.tensor(t, dtype=torch.long).to(self.device)
        # tensorh = torch.tensor(h, dtype=torch.long).to(self.device)
        batch_size = tensorq.size(0)
    
        databases = []
        for r in range(len(mdb)):
            ids = torch.LongTensor(mdb[r][0])
            values = torch.FloatTensor(mdb[r][1])
            map1 = torch.sparse.FloatTensor(ids.t(), values, mdb[r][2]).to(self.device) # (3007,3007)
            databases.append(map1)

        # print '-------------debug---------'

        u_l   = onehot(tensort, self.num_entity).unsqueeze(1).repeat(1,self.nhead,1) # (bs,1,3007)
        query = onehot(tensorq, self.num_entity).unsqueeze(1).repeat(1,self.nhead,1) # (bs,1,3007)

        query_rep = self.fc_q(query)  # bs,nhead,hids

        # for l in range(self.num_step):
        for al_att  in self.attention_layers:
            u_l_rep = self.fc1(u_l)  # bs,nhead,hids
            a_l = al_att(torch.cat([query,u_l],2).view(batch_size,-1)).view(batch_size, self.nhead,self.num_operator)
            a_l = nn.Softmax(2)(a_l) # (bs,24)

            database_results = []    
            for r in xrange(self.num_operator/2): # 12
                for op_matrix, op_attn in zip(
                                [databases[r], 
                                 databases[r].transpose(1,0)], # 
                                [a_l[:,:,r],  a_l[:,:,r+self.num_operator/2]]): # (bs,)
                    # M_R_k sum u_t
                    product = torch.matmul(u_l,op_matrix.to_dense())  # (bs,nhead,3007)
                    # a_k M_R_k sum b_t _u_t
                    sumat = product*op_attn.unsqueeze(2) # (bs,nhead,3007)
                    database_results.append(sumat.unsqueeze(1))
                # u_t 
            u_l = torch.sum(torch.cat(database_results,1),1, keepdim = False) # 128,head,3007

            if self.norm:
                u_l /= torch.max(torch.sum(u_l,2,
                    keepdim=True), torch.cuda.FloatTensor([self.thr]))


            if self.dropout > 0.:
                u_l = nn.Dropout(self.dropout)(u_l)

        predictions = nn.MaxPool2d((self.nhead,1))(u_l).squeeze()
        predictions /= torch.max(torch.sum(predictions,1,
                    keepdim=True), torch.cuda.FloatTensor([self.thr]))
        #predictions = torch.sum(u_l,1,keepdim=False)/(torch.sum(torch.sum(u_l,2),1,keepdim=True))
        # predictions = nn.Softmax(1)(self.fc2(self.fc1(u_l).view(batch_size,-1)))
        loss = - torch.mean(onehot(tensorh, self.num_entity) * torch.log(torch.max(predictions, \
                torch.cuda.FloatTensor([self.thr]))))
        # loss = self.criterion(predictions, tensorh)

        values,index_topk = torch.topk(predictions,self.top_k, sorted=False)      
        acc_res = torch.eq(index_topk, tensorh.unsqueeze(1))
        in_top = torch.sum(acc_res,1)
        # return loss, in_top, predictions, index_topk
        return values
 
    def get_predictions_given_queries(self, qq, hh, tt, mdb):
        loss, in_top, predictions, top_k = self.forward(qq, hh, tt, mdb)
        return in_top.detach().cpu().numpy(), predictions.detach().cpu().numpy()
  
 
class LogicLearning_b(nn.Module):
    """ 
    pytorch version
    """

    def __init__(self, option):
        super(LogicLearning_b, self).__init__()
        self.seed = option.seed        
        self.num_step = option.num_step
        self.num_layer = option.num_layer
        self.rnn_state_size = option.rnn_state_size
        
        self.norm = not option.no_norm
        self.thr = option.thr
        self.dropout = option.dropout
        self.learning_rate = option.learning_rate
        self.accuracy = option.accuracy
        self.top_k = option.top_k
        
        self.num_entity = option.num_entity
        self.num_operator = option.num_operator
        self.query_is_language = option.query_is_language
        
        if not option.query_is_language:
            self.num_query = option.num_query
            self.query_embed_size = option.query_embed_size       
        else:
            self.vocab_embed_size = option.vocab_embed_size
            self.query_embed_size = self.vocab_embed_size
            self.num_vocab = option.num_vocab
            self.num_query = num_vocab
            self.num_word = option.num_word    

        self.qembedding =  nn.Embedding(self.num_query+1,self.query_embed_size)       
        self.qrnn = nn.LSTM(self.query_embed_size, self.rnn_state_size,1,batch_first =True)

        self.att_rel = nn.Linear(self.num_entity*2,self.num_operator) 
        self.attention_layers = [ nn.Linear(self.num_entity*2, self.num_operator) for i in
                range(self.num_step)]
        # self.attentions_layers = []
        for i,attention in enumerate(self.attention_layers):
                self.add_module('attention_layer_{}_{}'.format(i,i), attention) # important, add module to torch
            # self.attentions_layers.append(self.attentions)


        self.criterion = nn.CrossEntropyLoss()

        self.device = torch.device("cuda" if torch.cuda.is_available() and not option.no_cuda else "cpu")
        self.to(self.device)
        # n_gpu = torch.cuda.device_count()

        print('-'*10+"Neual LP is built."+'-'*10)

    def forward(self, q,h,t, mdb):
        '''
        q,t,h: b_s,1 (list
        mdb: 12 maps
        '''
        batch_size = len(q)
        tensorq = torch.tensor(q, dtype=torch.long).to(self.device)
        tensort = torch.tensor(t, dtype=torch.long).to(self.device)
        tensorh = torch.tensor(h, dtype=torch.long).to(self.device)
    
        databases = []
        for r in range(len(mdb)):
            ids = torch.LongTensor(mdb[r][0])
            values = torch.FloatTensor(mdb[r][1])
            map1 = torch.sparse.FloatTensor(ids.t(), values, mdb[r][2]).to(self.device) # (3007,3007)
            databases.append(map1)

        # print '-------------debug---------'

        # u_l = onehot(tensort, self.num_entity) # (bs,1,3007)
        query = onehot(tensorq, self.num_entity) # (bs,1,3007)

        query_rep = self.fc_q(query).unsqueeze(2)  # bs,hids,1
        # u_l, actually the tails at first
        u_ls = onehot(tensort, self.num_entity).unsqueeze(1) # (bs,l,3007)


        # for l in range(self.num_step):
        for al_att  in self.attention_layers:
            u_ls_rep = self.fc1(u_ls)  # bs,l,hids
            b_l = nn.Softmax(1)(torch.bmm(u_ls_rep,query_rep)) # bs,l,1

            u_l_prime = torch.bmm(b_l.permute(0,2,1),u_ls) # bs,1,3007

            a_l = al_att(torch.cat([query_rep.permute(0,2,1),self.fc1(u_l_prime)],1)) #bs,1,num_operator
            a_l = nn.Softmax(2)(a_l) # (bs,1,24)

            database_results = []    
            for r in xrange(self.num_operator/2): # 12
                for op_matrix, op_attn in zip(
                                [databases[r], 
                                 databases[r].transpose(1,0)], # 
                                [a_l[:,r],  a_l[:,r+self.num_operator/2]]): # (bs,)
                    # M_R_k sum u_t
                    product = torch.matmul(u_l_prime,op_matrix.to_dense())  # (bs,1,3007)
                    # a_k M_R_k sum b_t _u_t
                    sumat = product*op_attn.unsqueeze(1)
                    database_results.append(sumat)
                # u_t 
            u_l = torch.sum(torch.cat(database_results,1),1, keepdim = True) # 128,1,3007

            if self.norm:
                u_l /= torch.max(torch.sum(u_l,2,
                    keepdim=True), torch.cuda.FloatTensor([self.thr]))


            if self.dropout > 0.:
                u_l = nn.Dropout(self.dropout)(u_l)

            u_ls = torch.cat( [u_ls, u_l],1) #(bs,t+1,3007)

        predictions = u_l 
        loss = - torch.mean(onehot(tensorh, self.num_entity) * torch.log(torch.max(predictions, \
                torch.cuda.FloatTensor([self.thr]))))
        # loss = self.criterion(predictions, tensorh)

        values,index_topk = torch.topk(predictions,self.top_k, sorted=False)      
        acc_res = torch.eq(index_topk, tensorh.unsqueeze(1))
        in_top = torch.sum(acc_res,1)
        return loss, in_top, predictions, index_topk
 
    def get_predictions_given_queries(self, qq, hh, tt, mdb):
        loss, in_top, predictions, top_k = self.forward(qq, hh, tt, mdb)
        return in_top.detach().cpu().numpy(), predictions.detach().cpu().numpy()

def onehot(data1, n_dimension):
    n_dim = data1.dim()
    batch_size = data1.size()[0]
    data = data1.view(-1,1)  
    y_onehot = torch.FloatTensor(data.size(0),n_dimension).zero_()
    ones = torch.FloatTensor(data.size()).fill_(1)

    if data.is_cuda:
        y_onehot = y_onehot.cuda()
        ones = ones.cuda()

    y_onehot.scatter_(1,data,ones)
    if n_dim ==1:
        return y_onehot.view(batch_size,n_dimension)
    elif n_dim ==2:
        return y_onehot.view(batch_size,-1,n_dimension)

